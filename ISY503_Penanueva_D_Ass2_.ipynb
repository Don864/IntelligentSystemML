{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLCLGZquX49T6l8f0DWcvb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Don864/IntelligentSystemML/blob/main/ISY503_Penanueva_D_Ass2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FFxntm5LHeG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ISY503 Intelligent Systems Assessment 2\n",
        "# Prepared by Donny Penanueva\n",
        "\n",
        "# ReadMe\n",
        "- The car data set we will be using in this lab is provided as a comma-separated file without a header row.\n",
        " - The data consists of 205 rows\n",
        " - 15 numeric_feature_names\n",
        " - 10 categorical_feature_names\n",
        "-  For each column to have a meaningful header name is provided.\n",
        "- Header 'price' is the target feature (y-axis)\n",
        "- This is a Google Colab file (.ipynb) which you can run by going to this website https://colab.google. From the colab website, you can also download this file as .py to run locally in your desired IDE, all dependencies need to install to make the code work properly.\n",
        "\n",
        "# Task 0\n",
        "  - This task prepares the data and validate the categories such as numeric and categorical features.\n",
        "  - The dataset contains 26 columns and the column 'price' (y-axis) is the target. While the 25 columns composed of 15 numeric features and 10 categorical features.\n",
        "  - Filling missing values with 0 can impact and may produce bias in the model.\n",
        "\n",
        "  - Numeric Feature Names:\n",
        "['symboling', 'normalized-losses', 'wheel-base', 'length', 'width', 'height', 'weight', 'engine-size', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'bore', 'stroke', 'compression-ratio']\n",
        "\n",
        "- Categorical Feature Names:\n",
        "['fuel-system', 'make', 'num-doors', 'engine-type', 'fuel-type', 'drive-wheels', 'body-style', 'num-cylinders', 'engine-location', 'aspiration']\n",
        "\n",
        "# Task 1\n",
        "Making the best model for numeric features without normalization allowed by changing some of hyperparamaters:\n",
        "\n",
        " - Learning rate: Changing the learning rate from 0.01 to 0.001. Setting the learning rate lower can avoid divergence and help stabilize the training.\n",
        "\n",
        " - Choice of optimizer: The optimizer used is Adam Optimizer for its adaptive learning rates, and improved performance. Tried to use Adagrad optimizer but did not get the best result, so Adam is the preferred  option for this model and dataset. When using the Gradient Descent Optimizer (SGD), the result has an Nan loss error , which makes it not suitable for this model of dataset.                \n",
        "\n",
        "- Hidden layer dimensions: Increased the hidden layer dimensions to [128, 64] to learn complex patterns in the data.\n",
        "\n",
        "- Batch size: For a small dataset 16 batch size is reasonable which can also help in avoiding overfitting.\n",
        "\n",
        "- Num training steps: The training steps used is 20000 which gives a better result. Initially tried to use 12000 and 15000 but it was giving a message to increase the steps.\n",
        "\n",
        "\n",
        " Hyperparamaters     value\n",
        "---------------     --------\n",
        " learning rate      .001\n",
        " batch size         16\n",
        " hidden layer       [128, 64]  \n",
        " num training steps 20000  \n",
        "\n",
        "\n",
        "# Task 2\n",
        " With the use of normalization the results are better and improved compared to task 1. By normalizing the model makes it more efficient and effective. Adam optimizer mostly works well with normalization features. Meanwhile Gradient Descent Optimizer (SGD) is not suitable for this model which is giving a Nan loss during training.\n",
        "\n",
        "\n",
        " # Task 3\n",
        " The categorical features is used in this task.\n",
        "\n",
        " # Task 4\n",
        " In this task all the features are consolidated to achieve the best results for this model. In this code \"numeric_only=True\" is added to avoid error when running the code below:\n",
        "normalizer_fn=lambda val: (val - x_df.mean(numeric_only=True)[feature_name]) / (epsilon + x_df.std(numeric_only=True)[feature_name]))\n",
        "\n"
      ],
      "metadata": {
        "id": "bqXZv3pfmrqU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y38V73EgVYwt"
      },
      "source": [
        "\n",
        "### Task 0\n",
        "print('## Task 0 Preparing the data')\n",
        "# Importing the dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Providing the names for the columns\n",
        "feature_names = ['symboling', 'normalized-losses', 'make', 'fuel-type',\n",
        "        'aspiration', 'num-doors', 'body-style', 'drive-wheels',\n",
        "        'engine-location', 'wheel-base', 'length', 'width', 'height', 'weight',\n",
        "        'engine-type', 'num-cylinders', 'engine-size', 'fuel-system', 'bore',\n",
        "        'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg',\n",
        "        'highway-mpg', 'price']\n",
        "\n",
        "\n",
        "# Load in the data from a CSV file that is comma separated.\n",
        "car_data = pd.read_csv('https://storage.googleapis.com/mledu-datasets/cars_data.csv',\n",
        "                        sep=',', names=feature_names, header=None, encoding='latin-1')\n",
        "\n",
        "\n",
        "LABEL = 'price'\n",
        "\n",
        "# Randomize the data.\n",
        "car_data = car_data.reindex(np.random.permutation(car_data.index))\n",
        "\n",
        "# Printing the number of examples = 205\n",
        "print(\"Data set loaded. Num examples: \", len(car_data))\n",
        "\n",
        "print(car_data.head())\n",
        "\n",
        "# Use type() to confirm that each column is of the expected type (numeric or object)\n",
        "for column in car_data.columns:\n",
        "    print(f\"Column '{column}' is of type: {type(car_data[column].iloc[0])}\")\n",
        "\n",
        "# Manually curate a list of numeric feature names based on the column types\n",
        "numeric_feature_names = ['symboling','normalized-losses','wheel-base', 'length', 'width', 'height', 'weight',\n",
        "                         'engine-size','horsepower','peak-rpm',\n",
        "                         'city-mpg','highway-mpg', 'bore','stroke','compression-ratio']\n",
        "categorical_feature_names = list(set(feature_names) - set(numeric_feature_names) - set([LABEL]))\n",
        "\n",
        "\n",
        "#To show the length of numeric_feature_names and categorical_feature_names\n",
        "print(\"\\nNumeric Feature Names Length:\")\n",
        "print(len(numeric_feature_names))\n",
        "print(\"\\nCategorical Feature Names Length:\")\n",
        "print(len(categorical_feature_names))\n",
        "\n",
        "\n",
        "# Confirm the lists\n",
        "print(\"\\nNumeric Feature Names:\")\n",
        "print(numeric_feature_names)\n",
        "print(\"\\nCategorical Feature Names:\")\n",
        "print(categorical_feature_names)\n",
        "\n",
        "# Testing if the correct solution will pass these assert statements.\n",
        "assert len(numeric_feature_names) == 15\n",
        "assert len(categorical_feature_names) == 10\n",
        "\n",
        "# Run to inspect numeric features.\n",
        "car_data[numeric_feature_names]\n",
        "\n",
        "# Run to inspect categorical features.\n",
        "car_data[categorical_feature_names]\n",
        "\n",
        "\n",
        "# # Coerce the numeric features to numbers. This is necessary because the model\n",
        "# # crashes because not all the values are numeric.\n",
        "for feature_name in numeric_feature_names + [LABEL]:\n",
        "  car_data[feature_name] = pd.to_numeric(car_data[feature_name], errors='coerce')\n",
        "\n",
        "# # Fill missing values with 0.\n",
        "car_data.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Task 1 Numeric feaures without normalization.\n",
        "# By using AdamOptimizer instead of GradientDescentOptimizer fixed the Nan loss error\n",
        "print('## Task 1 Numeric features without normalization')\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "print(numeric_feature_names)\n",
        "x_df = car_data[numeric_feature_names]\n",
        "y_series = car_data['price']\n",
        "\n",
        "# Create input_fn's so that the estimator knows how to read in your data.\n",
        "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=None,\n",
        "    shuffle=True)\n",
        "\n",
        "eval_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "# Feature columns allow the model to parse the data, perform common\n",
        "# preprocessing, and automatically generate an input layer for the tf.Estimator.\n",
        "model_feature_columns = [\n",
        "    tf.feature_column.numeric_column(feature_name) for feature_name in numeric_feature_names\n",
        "]\n",
        "print('model_feature_columns', model_feature_columns)\n",
        "\n",
        "# Use AdamOptimizer instead of GradientDescentOptimizer\n",
        "est = tf.estimator.DNNRegressor(\n",
        "    feature_columns=model_feature_columns,\n",
        "    hidden_units=[128, 64],   # Increase the size of the hidden layers\n",
        "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001), # Adjust the learning rate\n",
        "  )\n",
        "\n",
        "# TRAIN\n",
        "num_print_statements = 10\n",
        "num_training_steps = 20000\n",
        "for _ in range(num_print_statements):\n",
        "  est.train(train_input_fn, steps=num_training_steps // num_print_statements)\n",
        "  scores = est.evaluate(eval_input_fn)\n",
        "\n",
        "  # The `scores` dictionary has several metrics automatically generated by the\n",
        "  # canned Estimator.\n",
        "  # `average_loss` is the average loss for an individual example.\n",
        "  # `loss` is the summed loss for the batch.\n",
        "  # In addition to these scalar losses, you may find the visualization functions\n",
        "  # in the next cell helpful for debugging model quality.\n",
        "  print('scores', scores)\n",
        "\n",
        "# Visualize the data\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def scatter_plot_inference_grid(est, x_df, feature_names):\n",
        "  \"\"\"Plots the predictions of the model against each feature.\n",
        "\n",
        "  Args:\n",
        "    est: The trained tf.Estimator.\n",
        "    x_df: The pandas dataframe with the input data (used to create\n",
        "      predict_input_fn).\n",
        "    feature_names: An iterable of string feature names to plot.\n",
        "  \"\"\"\n",
        "  def scatter_plot_inference(axis,\n",
        "                             x_axis_feature_name,\n",
        "                             y_axis_feature_name,\n",
        "                             predictions):\n",
        "    \"\"\"Generate one subplot.\"\"\"\n",
        "    # Plot the real data in grey.\n",
        "    y_axis_feature_name = 'price'\n",
        "    axis.set_ylabel(y_axis_feature_name)\n",
        "    axis.set_xlabel(x_axis_feature_name)\n",
        "    axis.scatter(car_data[x_axis_feature_name],\n",
        "                 car_data[y_axis_feature_name],\n",
        "                 c='grey')\n",
        "\n",
        "    # Plot the predicted data in orange.\n",
        "    axis.scatter(car_data[x_axis_feature_name], predictions, c='orange')\n",
        "\n",
        "  predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "  predictions = [\n",
        "    x['predictions'][0]\n",
        "    for x in est.predict(predict_input_fn)\n",
        "  ]\n",
        "\n",
        "  num_cols = 3\n",
        "  num_rows = int(math.ceil(len(feature_names)/float(num_cols)))\n",
        "  f, axarr = plt.subplots(num_rows, num_cols)\n",
        "  size = 4.5\n",
        "  f.set_size_inches(num_cols*size, num_rows*size)\n",
        "\n",
        "  for i, feature_name in enumerate(numeric_feature_names):\n",
        "    axis = axarr[int(i/num_cols), i%num_cols]\n",
        "    scatter_plot_inference(axis, feature_name, 'price', predictions)\n",
        "  plt.show()\n",
        "\n",
        "scatter_plot_inference_grid(est, x_df, numeric_feature_names)\n",
        "\n",
        "\n",
        "# Task 2  with Normalization\n",
        "print('## Task 2 with Normalization')\n",
        "\n",
        "# This 1D visualization of each numeric feature might inform your normalization\n",
        "# decisions.\n",
        "for feature_name in numeric_feature_names:\n",
        "  car_data.hist(column=feature_name)\n",
        "\n",
        "\n",
        "# This does Z-score normalization since the distributions for most features looked\n",
        "# roughly normally distributed.\n",
        "\n",
        "# Z-score normalization subtracts the mean and divides by the standard deviation,\n",
        "# to give a roughly standard normal distribution (mean = 0, std = 1) under a\n",
        "# normal distribution assumption. Epsilon prevents divide by zero.\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "print(numeric_feature_names)\n",
        "x_df = car_data[numeric_feature_names]\n",
        "y_series = car_data['price']\n",
        "\n",
        "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=None,\n",
        "    shuffle=True)\n",
        "\n",
        "eval_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "# Epsilon prevents divide by zero.\n",
        "epsilon = 0.000001\n",
        "model_feature_columns = [\n",
        "    tf.feature_column.numeric_column(feature_name,\n",
        "                                     normalizer_fn=lambda val: (val - x_df.mean()[feature_name]) / (epsilon + x_df.std()[feature_name]))\n",
        "    for feature_name in numeric_feature_names\n",
        "]\n",
        "print('model_feature_columns', model_feature_columns)\n",
        "\n",
        "est = tf.estimator.DNNRegressor(\n",
        "    feature_columns=model_feature_columns,\n",
        "    hidden_units=[128, 64],\n",
        "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
        "  )\n",
        "\n",
        "# TRAIN\n",
        "num_print_statements = 10\n",
        "num_training_steps = 20000\n",
        "for _ in range(num_print_statements):\n",
        "  est.train(train_input_fn, steps=num_training_steps // num_print_statements)\n",
        "  scores = est.evaluate(eval_input_fn)\n",
        "\n",
        "\n",
        "  print('scores', scores)\n",
        "\n",
        "scatter_plot_inference_grid(est, x_df, numeric_feature_names)\n",
        "\n",
        "\n",
        "#Task 3\n",
        "# We have the full list of values that each feature takes on, and the list is\n",
        "# relatively small so we use categorical_column_with_vocabulary_list.\n",
        "\n",
        "print('## Task 3 ')\n",
        "batch_size = 16\n",
        "\n",
        "x_df = car_data[categorical_feature_names]\n",
        "y_series = car_data['price']\n",
        "\n",
        "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=None,\n",
        "    shuffle=True)\n",
        "\n",
        "eval_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "model_feature_columns = [\n",
        "    tf.feature_column.indicator_column(\n",
        "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "            feature_name, vocabulary_list=car_data[feature_name].unique()))\n",
        "    for feature_name in categorical_feature_names\n",
        "]\n",
        "print('model_feature_columns', model_feature_columns)\n",
        "\n",
        "est = tf.estimator.DNNRegressor(\n",
        "    feature_columns=model_feature_columns,\n",
        "    hidden_units=[128, 64],\n",
        "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
        "  )\n",
        "\n",
        "# TRAIN\n",
        "num_print_statements = 10\n",
        "num_training_steps = 20000\n",
        "for _ in range(num_print_statements):\n",
        "  est.train(train_input_fn, steps=num_training_steps // num_print_statements)\n",
        "  scores = est.evaluate(eval_input_fn)\n",
        "\n",
        "\n",
        "  print('scores', scores)\n",
        "\n",
        "\n",
        "\n",
        "# Task 4\n",
        "# This is a first pass at a model that uses all the features.\n",
        "print('## Task 4 Consolidate all the features')\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "x_df = car_data[numeric_feature_names + categorical_feature_names]\n",
        "y_series = car_data['price']\n",
        "\n",
        "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=None,\n",
        "    shuffle=True)\n",
        "\n",
        "eval_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "epsilon = 0.000001\n",
        "model_feature_columns = [\n",
        "    tf.feature_column.indicator_column(\n",
        "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "            feature_name, vocabulary_list=car_data[feature_name].unique()))\n",
        "    for feature_name in categorical_feature_names\n",
        "] + [\n",
        "    tf.feature_column.numeric_column(feature_name,\n",
        "                                     normalizer_fn=lambda val: (val - x_df.mean(numeric_only=True)[feature_name]) / (epsilon + x_df.std(numeric_only=True)[feature_name]))\n",
        "    for feature_name in numeric_feature_names\n",
        "]\n",
        "\n",
        "\n",
        "print('model_feature_columns', model_feature_columns)\n",
        "\n",
        "est = tf.estimator.DNNRegressor(\n",
        "    feature_columns=model_feature_columns,\n",
        "    hidden_units=[128, 64],\n",
        "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
        "  )\n",
        "\n",
        "# TRAIN\n",
        "num_print_statements = 10\n",
        "num_training_steps = 20000\n",
        "for _ in range(num_print_statements):\n",
        "  est.train(train_input_fn, steps=num_training_steps // num_print_statements)\n",
        "  scores = est.evaluate(eval_input_fn)\n",
        "\n",
        "\n",
        "  print('scores', scores)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}